{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c5710aa-ca04-493a-9822-9535d6f9a68f",
   "metadata": {},
   "source": [
    "Puesta en marcha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32b3a3b1-7420-4ac2-afcd-24b34675ec99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://44313117e328:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f301e5b7e10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c1c6772-2368-4481-b083-c90cabdfef67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime has 8.0 gigabytes of available RAM\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12e85c3-60a6-4166-930d-94d640d444d6",
   "metadata": {},
   "source": [
    "Además, podemos conseguir el grado de paralelismo, es decir, el número de cores empleados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1845470e-b1b6-462d-9243-a9f98e363055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fad7644-a69e-4e8a-9add-cf5a38b293cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "dataset_parquet = spark.read.parquet('dataset/yellow_tripdata_2017-01.parquet', header=True, inferSchema=True)\n",
    "#revisar repartion() y coalese()\n",
    "dataset_parquet.createOrReplaceTempView('tabledataset')\n",
    "dfsql = spark.sql(\"SELECT * FROM tabledataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "981d3ebf-5889-438c-99f7-521666f56bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1| 2017-01-01 00:32:05|  2017-01-01 00:37:48|              1|          1.2|         1|                 N|         140|         236|           2|        6.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         7.8|                NULL|       NULL|\n",
      "|       1| 2017-01-01 00:43:25|  2017-01-01 00:47:42|              2|          0.7|         1|                 N|         237|         140|           2|        5.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.3|                NULL|       NULL|\n",
      "|       1| 2017-01-01 00:49:10|  2017-01-01 00:53:53|              2|          0.8|         1|                 N|         140|         237|           2|        5.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.8|                NULL|       NULL|\n",
      "|       1| 2017-01-01 00:36:42|  2017-01-01 00:41:09|              1|          1.1|         1|                 N|          41|          42|           2|        6.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         7.3|                NULL|       NULL|\n",
      "|       1| 2017-01-01 00:07:41|  2017-01-01 00:18:16|              1|          3.0|         1|                 N|          48|         263|           2|       11.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        12.3|                NULL|       NULL|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To know the values of the dataframe\n",
    "dfsql.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b669e04-c50d-482b-ac3e-6aa85b8b39c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  Filas|\n",
      "+-------+\n",
      "|9710820|\n",
      "+-------+\n",
      "\n",
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: integer (nullable = true)\n",
      " |-- airport_fee: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check data types of each column\n",
    "filas = spark.sql(\"SELECT COUNT(*) as Filas FROM tabledataset\")\n",
    "filas.show()\n",
    "dfsql.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66e5e1ca-0be2-4ac0-a148-16c5a966cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check null values\n",
    "null_counts = spark.sql(\"\"\"SELECT'VendorID' AS VendorID,\n",
    "                        SUM(CASE WHEN VendorID IS NULL THEN 1 ELSE 0 END) AS ContCol1,\n",
    "                        'tpep_pickup_datetime' AS tpep_pickup_datetime,        \n",
    "                        SUM(CASE WHEN tpep_pickup_datetime IS NULL THEN 1 ELSE 0 END) AS ContCol2, \n",
    "                        'tpep_dropoff_datetime' AS tpep_dropoff_datetime,        \n",
    "                        SUM(CASE WHEN tpep_dropoff_datetime IS NULL THEN 1 ELSE 0 END) AS ContCol3, \n",
    "                        'passenger_count' AS passenger_count,        \n",
    "                        SUM(CASE WHEN passenger_count IS NULL THEN 1 ELSE 0 END) AS ContCol4, \n",
    "                        'trip_distance' AS trip_distance,        \n",
    "                        SUM(CASE WHEN trip_distance IS NULL THEN 1 ELSE 0 END) AS ContCol5, \n",
    "                        'RatecodeID' AS RatecodeID,        \n",
    "                        SUM(CASE WHEN RatecodeID IS NULL THEN 1 ELSE 0 END) AS ContCol6, \n",
    "                        'store_and_fwd_flag' AS store_and_fwd_flag,        \n",
    "                        SUM(CASE WHEN store_and_fwd_flag IS NULL THEN 1 ELSE 0 END) AS ContCol7, \n",
    "                        'PULocationID' AS PULocationID,        \n",
    "                        SUM(CASE WHEN PULocationID IS NULL THEN 1 ELSE 0 END) AS ContCol8, \n",
    "                        'DOLocationID' AS DOLocationID,        \n",
    "                        SUM(CASE WHEN DOLocationID IS NULL THEN 1 ELSE 0 END) AS ContCol9, \n",
    "                        'payment_type' AS payment_type,        \n",
    "                        SUM(CASE WHEN payment_type IS NULL THEN 1 ELSE 0 END) AS ContCol10, \n",
    "                        'fare_amount' AS fare_amount,        \n",
    "                        SUM(CASE WHEN fare_amount IS NULL THEN 1 ELSE 0 END) AS ContCol11, \n",
    "                        'extra' AS extra,        \n",
    "                        SUM(CASE WHEN extra IS NULL THEN 1 ELSE 0 END) AS ContCol12, \n",
    "                        'mta_tax' AS mta_tax,        \n",
    "                        SUM(CASE WHEN mta_tax IS NULL THEN 1 ELSE 0 END) AS ContCol13, \n",
    "                        'tip_amount' AS tip_amount,        \n",
    "                        SUM(CASE WHEN tip_amount IS NULL THEN 1 ELSE 0 END) AS ContCol14, \n",
    "                        'tolls_amount' AS tolls_amount,        \n",
    "                        SUM(CASE WHEN tolls_amount IS NULL THEN 1 ELSE 0 END) AS ContCol15, \n",
    "                        'improvement_surcharge' AS improvement_surcharge,        \n",
    "                        SUM(CASE WHEN improvement_surcharge IS NULL THEN 1 ELSE 0 END) AS ContCol16, \n",
    "                        'total_amount' AS total_amount,        \n",
    "                        SUM(CASE WHEN total_amount IS NULL THEN 1 ELSE 0 END) AS ContCol17,\n",
    "                        'congestion_surcharge' AS congestion_surcharge,        \n",
    "                        SUM(CASE WHEN congestion_surcharge IS NULL THEN 1 ELSE 0 END) AS ContCol18, \n",
    "                        'airport_fee' AS airport_fee,        \n",
    "                        SUM(CASE WHEN airport_fee IS NULL THEN 1 ELSE 0 END) AS ContCol19\n",
    "                        \n",
    "                        FROM tabledataset\"\"\"    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "396982a9-e8a2-4fd9-9fba-74992dcf75a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------------+--------+---------------------+--------+---------------+--------+-------------+--------+----------+--------+------------------+--------+------------+--------+------------+--------+------------+---------+-----------+---------+-----+---------+-------+---------+----------+---------+------------+---------+---------------------+---------+------------+---------+--------------------+---------+-----------+---------+\n",
      "|VendorID|ContCol1|tpep_pickup_datetime|ContCol2|tpep_dropoff_datetime|ContCol3|passenger_count|ContCol4|trip_distance|ContCol5|RatecodeID|ContCol6|store_and_fwd_flag|ContCol7|PULocationID|ContCol8|DOLocationID|ContCol9|payment_type|ContCol10|fare_amount|ContCol11|extra|ContCol12|mta_tax|ContCol13|tip_amount|ContCol14|tolls_amount|ContCol15|improvement_surcharge|ContCol16|total_amount|ContCol17|congestion_surcharge|ContCol18|airport_fee|ContCol19|\n",
      "+--------+--------+--------------------+--------+---------------------+--------+---------------+--------+-------------+--------+----------+--------+------------------+--------+------------+--------+------------+--------+------------+---------+-----------+---------+-----+---------+-------+---------+----------+---------+------------+---------+---------------------+---------+------------+---------+--------------------+---------+-----------+---------+\n",
      "|VendorID|       0|tpep_pickup_datetime|       0| tpep_dropoff_date...|       0|passenger_count|       0|trip_distance|       0|RatecodeID|       0|store_and_fwd_flag|       0|PULocationID|       0|DOLocationID|       0|payment_type|        0|fare_amount|        0|extra|        0|mta_tax|        0|tip_amount|        0|tolls_amount|        0| improvement_surch...|        0|total_amount|        0|congestion_surcharge|  9710820|airport_fee|  9710820|\n",
      "+--------+--------+--------------------+--------+---------------------+--------+---------------+--------+-------------+--------+----------+--------+------------------+--------+------------+--------+------------+--------+------------+---------+-----------+---------+-----+---------+-------+---------+----------+---------+------------+---------+---------------------+---------+------------+---------+--------------------+---------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3342cc24-c127-47da-a402-b25fb03d5fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As all the values of congestion_surcharge and airport_fee are null, in SQL you cannot delete, therefore\n",
    "#select only the necessary columns\n",
    "dfsqlreduce = spark.sql(\"\"\"SELECT VendorID,\n",
    "                                  tpep_pickup_datetime,\n",
    "                                  tpep_dropoff_datetime,\n",
    "                                  passenger_count,\n",
    "                                  trip_distance,\n",
    "                                  RatecodeID,\n",
    "                                  store_and_fwd_flag,\n",
    "                                  PULocationID,\n",
    "                                  DOLocationID,\n",
    "                                  payment_type,\n",
    "                                  fare_amount,\n",
    "                                  extra,\n",
    "                                  mta_tax,\n",
    "                                  tip_amount,\n",
    "                                  tolls_amount,\n",
    "                                  improvement_surcharge,\n",
    "                                  total_amount\n",
    "                            FROM tabledataset    \n",
    "                        \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175760f7-a216-4471-ad8b-d5c7f064508e",
   "metadata": {},
   "source": [
    "Velocidad media de los taxis en función de la hora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7838483d-862a-4c33-8c96-1d07efde65d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's calculate the difference of time between the pick-up and the drop-off in seconds\n",
    "#Difference DateTime\n",
    "#Velocity\n",
    "#Hour from tpep_pickup_datetime\n",
    "#Hour from tpep_dropoff_datetime\n",
    "dfsqlreduce = spark.sql(\"\"\"SELECT *,\n",
    "                                (unix_timestamp(tpep_dropoff_datetime) - unix_timestamp(tpep_pickup_datetime)) AS difference_datetime,\n",
    "                                trip_distance/(( (unix_timestamp(tpep_dropoff_datetime) - unix_timestamp(tpep_pickup_datetime))/60 )/60) AS velocity,\n",
    "                                HOUR(tpep_pickup_datetime) AS pickup_hour,\n",
    "                                HOUR(tpep_dropoff_datetime) AS dropoff_hour\n",
    "                            FROM tabledataset\n",
    "                            WHERE tpep_dropoff_datetime >= tpep_pickup_datetime\n",
    "                        \"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "97482301-1ddc-46bf-9f66-8f4ba1f274ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-------------------+-----------------+-----------+------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|difference_datetime|         velocity|pickup_hour|dropoff_hour|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-------------------+-----------------+-----------+------------+\n",
      "|       1| 2017-01-01 00:32:05|  2017-01-01 00:37:48|              1|          1.2|         1|                 N|         140|         236|           2|        6.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         7.8|                NULL|       NULL|                343|12.59475218658892|          0|           0|\n",
      "|       1| 2017-01-01 00:43:25|  2017-01-01 00:47:42|              2|          0.7|         1|                 N|         237|         140|           2|        5.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.3|                NULL|       NULL|                257| 9.80544747081712|          0|           0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-------------------+-----------------+-----------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsqlreduce.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9654927e-ae12-4857-ad9c-8675dfcdc79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the velocity avg of each hour\n",
    "#dfsqlreduce_avg = dfsqlreduce\n",
    "dfsqlreduce.createOrReplaceTempView('tabledataset2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1aec2dd5-54f5-4780-accb-baee69087c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsqlreduce_avg = spark.sql(\"\"\"SELECT \n",
    "                                pickup_hour,\n",
    "                                avg(velocity) as hourly_average        \n",
    "                            FROM tabledataset2\n",
    "                            GROUP BY pickup_hour\n",
    "                            \"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861cb29c-2c1e-4c5e-bd08-b8d107ebeef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
